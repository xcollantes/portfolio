---
title: "Belva AI: Building LLM Microservices Infrastructure"
cardDescription: "Developed API for LLM microservices."
imagePath: ""
cardPageLink: "articles/belva-ai"
cardButtonText: "See more"
author: "Xavier Collantes"
dateWritten: 2025-03-20
dateLastUpdated: 2025-07-14
articleType: WORKEXP
tagIds:
  - python
  - llm
  - api
  - docker
  - kubernetes
---

At Belva AI I served as a Software Engineer building robust
microservices infrastructure for AI-powered applications.

### Technical Development and API Architecture

During my time at Belva AI, I was instrumental in developing the backend
infrastructure that powered our AI services:

- Developed 25+ REST API endpoints across 10+ different LLM microservices using
  FastAPI, Nginx, creating a comprehensive and scalable API ecosystem
- Implemented event-driven architecture using Kafka for asynchronous processing
  and communication between microservices
- Built data persistence layers with MongoDB, optimizing for both performance
  and flexibility in storing AI-generated content and user data
- Containerized all services using Docker and orchestrated deployments with
  Kubernetes to ensure consistent environments across development and production

### Database Architecture and Design

I played a key role in the architectural planning and implementation of database
solutions:

- Designed and architected database backend options for a
  user-facing AI-driven full-stack tool
- Prepared detailed cost-benefit analyses for each database solution, presenting
  tradeoffs to the CTO
- Implemented the selected database architecture, ensuring it met performance,
  scalability, and reliability requirements
- Optimized query patterns and established proper indexing strategies to
  maintain fast response times even as data volumes grew

### Technical Stack and Skills Applied

My work at Belva AI leveraged a modern technology stack including:

- Python with FastAPI for efficient API development
- Ollama for testing LLM models against each other
- Kafka for message streaming and event processing
- MongoDB for flexible document storage
- Docker and Kubernetes for containerization and orchestration
- Microservices architecture design and implementation
- CI/CD pipeline configuration for automated testing and deployment

This experience at Belva AI deepened my expertise in building distributed
systems for AI applications, particularly in designing and implementing scalable
infrastructure that can handle the unique demands of LLM-based services.
